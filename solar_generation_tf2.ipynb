{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[gpu_id], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_display = 2000\n",
    "iter_save = 10000\n",
    "iter_max = 500000\n",
    "\n",
    "\n",
    "name_input = 'hmi_M_720s'\n",
    "is_aia_input = False\n",
    "\n",
    "name_output = 'aia_304'\n",
    "is_aia_output = True\n",
    "\n",
    "csv_train = '/home/park_e/datasets/solar_generation_train.csv'\n",
    "csv_validation = '/home/park_e/datasets/solar_generation_validation.csv'\n",
    "csv_test = '/home/park_e/datasets/solar_generation_test.csv'\n",
    "\n",
    "root_save = '/userhome/park_e/solar_generation_tf2'\n",
    "\n",
    "isize = 1024\n",
    "rsun=392\n",
    "ch_input = 1\n",
    "ch_output = 1\n",
    "\n",
    "bsize = 1\n",
    "do_shake = True\n",
    "\n",
    "layer_max_d = 3\n",
    "\n",
    "lim_hmi = 1000.\n",
    "lim_aia = 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode1 = 'unet_cgan%d'%(layer_max_d)\n",
    "mode2 = '%s.%s'%(name_input, name_output)\n",
    "\n",
    "root_model = '%s/%s/%s/model' % (root_save, mode1, mode2)\n",
    "root_validation = '%s/%s/%s/validation' % (root_save, mode1, mode2)\n",
    "root_test = '%s/%s/%s/test' % (root_save, mode1, mode2)\n",
    "\n",
    "import os\n",
    "os.makedirs(root_model, exist_ok=True)\n",
    "os.makedirs(root_validation, exist_ok=True)\n",
    "os.makedirs(root_test, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "total_train = read_csv(csv_train)\n",
    "list_train_input = total_train[name_input]\n",
    "list_train_output = total_train[name_output]\n",
    "assert len(list_train_input) == len(list_train_output)\n",
    "list_train = list(zip(list_train_input, list_train_output))\n",
    "nb_train = len(list_train)\n",
    "\n",
    "total_validation = read_csv(csv_validation)\n",
    "list_validation = total_validation[name_input]\n",
    "nb_validation = len(list_validation)\n",
    "\n",
    "total_test = read_csv(csv_test)\n",
    "list_test = total_test[name_input]\n",
    "nb_test = len(list_test)\n",
    "\n",
    "print(nb_train, nb_validation, nb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import rescale, bytescale\n",
    "from imageio import imsave\n",
    "from random import shuffle\n",
    "\n",
    "X = np.arange(isize)[:, None]\n",
    "Y = np.arange(isize)[None, :]\n",
    "XY = np.sqrt((X-isize/2.)**2. + (Y-isize/2.)**2.)\n",
    "cfilter = np.where(XY>rsun*0.99)\n",
    "\n",
    "class make_tensor():\n",
    "    def __init__(self, is_aia, ch):\n",
    "        self.is_aia = is_aia\n",
    "        self.ch = ch\n",
    "    def __call__(self, x):\n",
    "        x = np.load(x)\n",
    "        if self.is_aia :\n",
    "            x = np.log2((x+1.).clip(1., 2.**lim_aia))\n",
    "            x = rescale(x, imin=0., imax=14., omin=-1, omax=1)\n",
    "            x[cfilter]=-1\n",
    "        else :\n",
    "            x = x.clip(-lim_hmi, lim_hmi) / lim_hmi\n",
    "            x[cfilter]=-1\n",
    "        x.shape = (1, isize, isize, self.ch)\n",
    "        return x\n",
    "\n",
    "class make_result():\n",
    "    def __init__(self, is_aia, ch):\n",
    "        self.is_aia = is_aia\n",
    "        self.ch = ch\n",
    "    def __call__(self, x):\n",
    "        x.shape = (isize, isize, self.ch) if self.ch != 1 else (isize, isize)\n",
    "        if self.is_aia :\n",
    "            result_npy = rescale(x, imin=-1., imax=1., omin=0., omax=lim_aia)\n",
    "            result_npy = 2**result_npy - 1\n",
    "            result_png = bytescale(x, imin=-1., imax=1.)\n",
    "        else :\n",
    "            result_npy = x*lim_hmi\n",
    "            result_png = result_npy.clip(-100, 100)\n",
    "            result_png = bytescale(result_png, imin=-100, imax=100)\n",
    "        return result_npy, result_png\n",
    "\n",
    "def shake_tensor(batch_A, batch_B):\n",
    "    pad = int(isize/64) - 1\n",
    "    x, y = np.random.randint(2*pad+1), np.random.randint(2*pad+1)\n",
    "    batch_A = np.pad(batch_A, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=-1)\n",
    "    batch_B = np.pad(batch_B, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=-1)\n",
    "    batch_A = batch_A[:, x:x+isize, y:y+isize, :]\n",
    "    batch_B = batch_B[:, x:x+isize, y:y+isize, :]\n",
    "    return batch_A, batch_B\n",
    "\n",
    "make_tensor_input = make_tensor(is_aia_input, ch_input)\n",
    "make_tensor_output = make_tensor(is_aia_output, ch_output)\n",
    "make_result_output = make_result(is_aia_output, ch_output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_generator():\n",
    "    epoch = i = 0\n",
    "    tmpsize = None\n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else bsize\n",
    "        if i + size > nb_train :\n",
    "            shuffle(list_train)\n",
    "            i = 0\n",
    "            epoch += 1\n",
    "        batch_A = np.concatenate([make_tensor_input(list_train[j][0]) for j in range(i, i+size)], 0)\n",
    "        batch_B = np.concatenate([make_tensor_output(list_train[j][1]) for j in range(i, i+size)], 0)\n",
    "        if do_shake :\n",
    "            batch_A, batch_B = shake_tensor(batch_A, batch_B)\n",
    "        i += size\n",
    "        tmpsize = yield epoch, batch_A, batch_B\n",
    "\n",
    "train_batch = train_batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks_tf2 import unet_generator, patch_discriminator\n",
    "network_G = unet_generator(isize, ch_input, ch_output)\n",
    "network_D = patch_discriminator(isize, ch_input, ch_output, layer_max_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_object(target, output):\n",
    "    return -tf.math.reduce_mean(tf.math.log(output+1e-12)*target+tf.math.log(1-output+1e-12)*(1-target))\n",
    "#loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_Dis(output_D_real, output_D_fake):\n",
    "    loss_D_real = loss_object(target=tf.ones_like(output_D_real), output=output_D_real)\n",
    "    loss_D_fake = loss_object(target=tf.zeros_like(output_D_fake), output=output_D_fake)\n",
    "    return (loss_D_real+loss_D_fake)/2.\n",
    "\n",
    "def loss_Gen(output_D_fake, fake_B, real_B):\n",
    "    loss_G_fake = loss_object(target=tf.ones_like(output_D_fake), output=output_D_fake)\n",
    "    loss_L = tf.reduce_mean(tf.abs(real_B - fake_B))\n",
    "    return loss_G_fake, loss_L\n",
    "\n",
    "optimizer_D = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "optimizer_G = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_A, real_B):\n",
    "    with tf.GradientTape() as tape_G, tf.GradientTape() as tape_D:\n",
    "        fake_B = network_G(real_A, training=True)\n",
    "        output_D_real = network_D([real_A, real_B], training=True)\n",
    "        output_D_fake = network_D([real_A, fake_B], training=True)\n",
    "\n",
    "        loss_G_fake, loss_L = loss_Gen(output_D_fake, fake_B, real_B)\n",
    "        loss_D = loss_Dis(output_D_real, output_D_fake)\n",
    "        loss_G = loss_G_fake + loss_L*100.\n",
    "\n",
    "    gradient_G = tape_G.gradient(loss_G, network_G.trainable_variables)\n",
    "    gradient_D = tape_D.gradient(loss_D, network_D.trainable_variables)\n",
    "\n",
    "    optimizer_G.apply_gradients(zip(gradient_G, network_G.trainable_variables))\n",
    "    optimizer_D.apply_gradients(zip(gradient_D, network_D.trainable_variables))\n",
    "\n",
    "    return loss_D, loss_G_fake, loss_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "TM = time.localtime(time.time())\n",
    "t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "\n",
    "print('\\n--------------------------------\\n')\n",
    "\n",
    "print('\\n%s : Now start below session!\\n'%(t_now))\n",
    "print('Model mode : %s'%mode1)\n",
    "print('Data mode : %s'%mode2)\n",
    "print('Model save path: %s'%(root_model))\n",
    "print('Validation snap save path: %s'%(root_validation))\n",
    "print('Test result save path: %s'%(root_test))\n",
    "print('# of train, validation, and test datasets : %d, %d, %d'%(nb_train, nb_validation, nb_test))\n",
    "\n",
    "print('\\n--------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "epoch = iter_gen = 0\n",
    "err_D = err_D_sum = err_D_mean = 0\n",
    "err_G = err_G_sum = err_G_mean = 0\n",
    "err_L = err_L_sum = err_L_mean = 0\n",
    "\n",
    "while iter_gen <= iter_max :\n",
    "\n",
    "    epoch, train_A, train_B = next(train_batch)\n",
    "    train_A = tf.cast(train_A, tf.float32)\n",
    "    train_B = tf.cast(train_B, tf.float32)\n",
    "    \n",
    "    err_D, err_G, err_L = train_step(train_A, train_B)\n",
    "    \n",
    "    err_D_sum += err_D\n",
    "    err_G_sum += err_G\n",
    "    err_L_sum += err_L\n",
    "    \n",
    "    iter_gen += bsize\n",
    "    \n",
    "    if iter_gen % iter_display == 0:\n",
    "\n",
    "        err_D_mean = err_D_sum/iter_display\n",
    "        err_G_mean = err_G_sum/iter_display\n",
    "        err_L_mean = err_L_sum/iter_display\n",
    "        message1 = '[%d][%d/%d]' % (epoch, iter_gen, iter_max)\n",
    "        message2 = 'LOSS_D: %5.3f LOSS_G: %5.3f LOSS_L: %5.3f T: %dsec/%dits' % (err_D_mean, err_G_mean, err_L_mean, time.time()-t0, iter_gen)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s %s'%(t_now, message1, message2))\n",
    "\n",
    "        err_L_sum, err_G_sum, err_D_sum = 0, 0, 0\n",
    "        t0 = time.time()\n",
    "\n",
    "    if iter_gen % iter_save == 0:\n",
    "        \n",
    "        dst_model = '%s/%s.%s.%07d'%(root_model, mode1, mode2, iter_gen)\n",
    "        network_G.save('%s.G.h5'%(dst_model))\n",
    "        network_D.save('%s.D.h5'%(dst_model))\n",
    "        message3 = 'network_G and network_D are saved under %s'%(root_model)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s'%(t_now, message3))\n",
    "\n",
    "        path_validation = '%s/iter_%07d'%(root_validation, iter_gen)\n",
    "        os.makedirs(path_validation, exist_ok=True)\n",
    "        for file_A in list_validation :\n",
    "            real_A = make_tensor_input(file_A)\n",
    "            real_A = tf.cast(real_A, tf.float32)\n",
    "            fake_B = network_G.predict(real_A)\n",
    "            fake_B, fake_B_png = make_result_output(fake_B)\n",
    "            name_save = '%s.%s.%07d.%s'%(mode1, mode2, iter_gen, file_A.split('/')[-1][-23:-4])\n",
    "            np.save('%s/%s.npy'%(path_validation, name_save), fake_B)\n",
    "            imsave('%s/%s.png'%(path_validation, name_save), fake_B_png)\n",
    "        message4 = 'Validation snaps (%d images) are saved in %s'%(nb_validation, path_validation)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s'%(t_now, message4))\n",
    "        \n",
    "        path_test = '%s/iter_%07d'%(root_test, iter_gen)\n",
    "        os.makedirs(path_test, exist_ok=True)\n",
    "        for file_A in list_test :\n",
    "            real_A = make_tensor_input(file_A)\n",
    "            real_A = tf.cast(real_A, tf.float32)\n",
    "            fake_B = network_G.predict(real_A)\n",
    "            fake_B, fake_B_png = make_result_output(fake_B)\n",
    "            name_save = '%s.%s.%07d.%s'%(mode1, mode2, iter_gen, file_A.split('/')[-1][-23:-4])\n",
    "            np.save('%s/%s.npy'%(path_test, name_save), fake_B)\n",
    "            imsave('%s/%s.png'%(path_test, name_save), fake_B_png)\n",
    "        message5 = 'Test snaps (%d images) are saved in %s'%(nb_test, path_test)\n",
    "        TM = time.localtime(time.time())\n",
    "        t_now = '%04d-%02d-%02d %02d:%02d:%02d'%(TM.tm_year, TM.tm_mon, TM.tm_mday, TM.tm_hour, TM.tm_min, TM.tm_sec)\n",
    "        print('%s: %s'%(t_now, message5))        \n",
    "        \n",
    "        t0 = time.time()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
